{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18824,"status":"ok","timestamp":1663773929493,"user":{"displayName":"‰∏≠Áî∞ÂÖâÈöÜ","userId":"06690068059315868411"},"user_tz":-540},"id":"SQXgVjJ4PZHc","outputId":"a25af516-2456-4079-d95b-8f851451f7d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1663773929494,"user":{"displayName":"‰∏≠Áî∞ÂÖâÈöÜ","userId":"06690068059315868411"},"user_tz":-540},"id":"FygRSaOeS6jS","outputId":"92cdbd19-5137-41db-90d6-06e53a7b5777"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/substrate\n"]}],"source":["import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/substrate')\n","\n","!pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17031,"status":"ok","timestamp":1663773946519,"user":{"displayName":"‰∏≠Áî∞ÂÖâÈöÜ","userId":"06690068059315868411"},"user_tz":-540},"id":"BcRcUYo1S8JX","outputId":"fa68e1ff-d69a-4d7f-e9f5-3292fad9a168"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 509 kB 7.2 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.8 MB 57.8 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 707 kB 66.5 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 419 kB 74.3 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181 kB 66.6 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158 kB 69.4 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 2.0 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 72.4 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 69.1 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 70.5 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 77.5 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 62.2 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 78.7 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 69.9 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 156 kB 79.7 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.9 MB 51.6 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.2+zzzcolab20220719082949 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.10.0 which is incompatible.\u001b[0m\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151 kB 7.2 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117 kB 62.0 MB/s \n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79 kB 8.6 MB/s \n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q timm wandb pytorch_lightning torchmetrics tensorboard\n","!pip install -q hydra-core --upgrade"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28415,"status":"ok","timestamp":1663773975203,"user":{"displayName":"‰∏≠Áî∞ÂÖâÈöÜ","userId":"06690068059315868411"},"user_tz":-540},"id":"ISBww_TKTBHN","outputId":"6aec5def-c5dd-47d0-bba7-1eca18ad3263"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!wandb login"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N44D20x_TFdg","executionInfo":{"status":"ok","timestamp":1663774202967,"user_tz":-540,"elapsed":227770,"user":{"displayName":"‰∏≠Áî∞ÂÖâÈöÜ","userId":"06690068059315868411"}},"outputId":"ce8a1ee4-9604-4a79-c014-c2bcdf1f683a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33minsilicomab\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Colab Notebooks/substrate/wandb/run-20220921_152626-3lbfrnwb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbumbling-cloud-41\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/insilicomab/substrate\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/insilicomab/substrate/runs/3lbfrnwb\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py:353: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\n","Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n","  warnings.warn(*args, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:448: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab Notebooks/substrate/wandb/run-20220921_152626-3lbfrnwb/files exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","Epoch 0:   0% 0/73 [00:00<?, ?it/s] /usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n","  warnings.warn(*args, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  55% 40/73 [00:48<00:39,  1.20s/it, loss=0.661, v_num=rnwb]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  82% 60/73 [01:08<00:14,  1.15s/it, loss=0.661, v_num=rnwb]\n","Validation DataLoader 0:  91% 20/22 [00:17<00:01,  1.15it/s]\u001b[A\n","Epoch 0: 100% 73/73 [01:21<00:00,  1.11s/it, loss=0.642, v_num=rnwb, val_loss=0.638, val_Accuracy=0.655, val_Precision=0.328, val_Recall=0.500, val_Specificity=0.500, val_F1Score=0.396, val_FBetaScore=0.352, val_AUROC=0.638]\n","Epoch 0: 100% 73/73 [01:21<00:00,  1.11s/it, loss=0.642, v_num=rnwb, val_loss=0.638, val_Accuracy=0.655, val_Precision=0.328, val_Recall=0.500, val_Specificity=0.500, val_F1Score=0.396, val_FBetaScore=0.352, val_AUROC=0.638, train_loss=0.690, train_Accuracy=0.507, train_Precision=0.499, train_Recall=0.499, train_Specificity=0.499, train_F1Score=0.490, train_FBetaScore=0.494, train_AUROC=0.484]tcmalloc: large alloc 1196359680 bytes == 0xde66a000 @  0x7fd118766615 0x58e046 0x4f2e5e 0x58ee1f 0x58f096 0x7fd0e020007f 0x7fd0e01d0974 0x7fd0b9ed1ef5 0x7fd0b9ecc441 0x7fd0b9ed3549 0x7fd0e01d0f3b 0x7fd0dfe5af61 0x58ec54 0x58fc01 0x51b7fd 0x5b4a3e 0x58f49e 0x51740e 0x5b41c5 0x58f49e 0x51b221 0x58f2a7 0x51740e 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e\n","Epoch 1:  55% 40/73 [00:27<00:22,  1.44it/s, loss=0.628, v_num=rnwb, val_loss=0.638, val_Accuracy=0.655, val_Precision=0.328, val_Recall=0.500, val_Specificity=0.500, val_F1Score=0.396, val_FBetaScore=0.352, val_AUROC=0.638, train_loss=0.690, train_Accuracy=0.507, train_Precision=0.499, train_Recall=0.499, train_Specificity=0.499, train_F1Score=0.490, train_FBetaScore=0.494, train_AUROC=0.484]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/22 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:  82% 60/73 [00:39<00:08,  1.50it/s, loss=0.628, v_num=rnwb, val_loss=0.638, val_Accuracy=0.655, val_Precision=0.328, val_Recall=0.500, val_Specificity=0.500, val_F1Score=0.396, val_FBetaScore=0.352, val_AUROC=0.638, train_loss=0.690, train_Accuracy=0.507, train_Precision=0.499, train_Recall=0.499, train_Specificity=0.499, train_F1Score=0.490, train_FBetaScore=0.494, train_AUROC=0.484]\n","Validation DataLoader 0:  91% 20/22 [00:08<00:00,  2.27it/s]\u001b[A\n","Epoch 1: 100% 73/73 [00:45<00:00,  1.61it/s, loss=0.604, v_num=rnwb, val_loss=0.617, val_Accuracy=0.655, val_Precision=0.328, val_Recall=0.500, val_Specificity=0.500, val_F1Score=0.396, val_FBetaScore=0.352, val_AUROC=0.706, train_loss=0.690, train_Accuracy=0.507, train_Precision=0.499, train_Recall=0.499, train_Specificity=0.499, train_F1Score=0.490, train_FBetaScore=0.494, train_AUROC=0.484]\n","Epoch 2:  55% 40/73 [00:28<00:23,  1.42it/s, loss=0.598, v_num=rnwb, val_loss=0.617, val_Accuracy=0.655, val_Precision=0.328, val_Recall=0.500, val_Specificity=0.500, val_F1Score=0.396, val_FBetaScore=0.352, val_AUROC=0.706, train_loss=0.635, train_Accuracy=0.655, train_Precision=0.328, train_Recall=0.500, train_Specificity=0.500, train_F1Score=0.396, train_FBetaScore=0.352, train_AUROC=0.653]\n","Validation: 0it [00:00, ?it/s]\u001b[ATraceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\n","    return trainer_fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n","    results = self._run(model, ckpt_path=self.ckpt_path)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\n","    results = self._run_stage()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\n","    return self._run_train()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\n","    self.fit_loop.run()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 271, in advance\n","    self._outputs = self.epoch_loop.run(self._data_fetcher)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 201, in run\n","    self.on_advance_end()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 241, in on_advance_end\n","    self._run_validation()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 299, in _run_validation\n","    self.val_loop.run()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 155, in advance\n","    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n","    self.advance(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 127, in advance\n","    batch = next(data_fetcher)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 184, in __next__\n","    return self.fetching_function()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 263, in fetching_function\n","    self._fetch_next_batch(self.dataloader_iter)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\", line 277, in _fetch_next_batch\n","    batch = next(iterator)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1359, in _next_data\n","    idx, data = self._get_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1315, in _get_data\n","    success, data = self._try_get_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1163, in _try_get_data\n","    data = self._data_queue.get(timeout=timeout)\n","  File \"/usr/lib/python3.7/queue.py\", line 179, in get\n","    self.not_empty.wait(remaining)\n","  File \"/usr/lib/python3.7/threading.py\", line 300, in wait\n","    gotit = waiter.acquire(True, timeout)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"train.py\", line 112, in <module>\n","    main()\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/main.py\", line 95, in decorated_main\n","    config_name=config_name,\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 396, in _run_hydra\n","    overrides=overrides,\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 453, in _run_app\n","    lambda: hydra.run(\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 213, in run_and_report\n","    return func()\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/utils.py\", line 456, in <lambda>\n","    overrides=overrides,\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py\", line 127, in run\n","    configure_logging=with_log_configuration,\n","  File \"/usr/local/lib/python3.7/dist-packages/hydra/core/utils.py\", line 186, in run_job\n","    ret.return_value = task_function(task_cfg)\n","  File \"train.py\", line 108, in main\n","    trainer.fit(net, datamodule=datamodule)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 697, in fit\n","    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 653, in _call_and_handle_interrupt\n","    rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/rank_zero.py\", line 32, in wrapped_fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/rank_zero.py\", line 92, in rank_zero_warn\n","    _warn(message, stacklevel=stacklevel, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/rank_zero.py\", line 86, in _warn\n","    warnings.warn(message, stacklevel=stacklevel, **kwargs)\n","  File \"/usr/lib/python3.7/warnings.py\", line 112, in _showwarnmsg\n","    _showwarnmsg_impl(msg)\n","  File \"/usr/lib/python3.7/warnings.py\", line 28, in _showwarnmsg_impl\n","    text = _formatwarnmsg(msg)\n","  File \"/usr/lib/python3.7/warnings.py\", line 128, in _formatwarnmsg\n","    return _formatwarnmsg_impl(msg)\n","  File \"/usr/lib/python3.7/warnings.py\", line 42, in _formatwarnmsg_impl\n","    line = linecache.getline(msg.filename, msg.lineno)\n","  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n","    lines = getlines(filename, module_globals)\n","  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n","    return updatecache(filename, module_globals)\n","  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n","    with tokenize.open(fullname) as fp:\n","  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n","    encoding, lines = detect_encoding(buffer.readline)\n","  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n","    first = read_or_stop()\n","  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n","    return readline()\n","KeyboardInterrupt\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n","^C\n"]}],"source":["!python train.py"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOUVF5Y3kEvt7VHSArDpA+m"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}